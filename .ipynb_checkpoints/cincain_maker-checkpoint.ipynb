{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys, io, nltk, random\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import cmudict\n",
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finds similar words from random text in nlk books\n",
    "def similar_random(word):\n",
    "    similar_text = f\".similar('{word}')\"\n",
    "    text_number = random.randint(1,9)\n",
    "    program = f\"text{text_number}{similar_text}\"\n",
    "    exec(program)\n",
    "    \n",
    "# captures and returns similar_random text as list \n",
    "def similar_to_str(word):\n",
    "    \n",
    "    # redirect sys.stdout to a buffer\n",
    "    stdout = sys.stdout\n",
    "    sys.stdout = io.StringIO()\n",
    "\n",
    "    # similar_random calls a print(), which we'll capture as a variable\n",
    "    similar_random(word)\n",
    "\n",
    "    # get output and restore sys.stdout\n",
    "    output = sys.stdout.getvalue()\n",
    "    sys.stdout = stdout\n",
    "\n",
    "    \n",
    "    # removes '\\n' and 'No matches' entries returns a list\n",
    "    output = output.replace('\\n','')\n",
    "    output = output.replace('No matches','')\n",
    "    output = output.split()\n",
    "    return(output)\n",
    "\n",
    "# repeats similar_to_str until output > int(default=10) or (default=10) attempts\n",
    "def repeat_similar_to_str(word, output=10, attempts=10):\n",
    "    similar_list = []\n",
    "    for i in range(output):\n",
    "        similar_list = similar_list + similar_to_str(word)\n",
    "        if len(similar_list) > output:\n",
    "            break\n",
    "    return(similar_list)\n",
    "        \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def syl_count_pos(word):\n",
    "    d = cmudict.dict()\n",
    "    syl_count = [len(list(y for y in x if y[-1].isdigit())) for x in d[word.lower()]][0]\n",
    "    token = nltk.word_tokenize(word)\n",
    "    pos = nltk.pos_tag(token)[0]\n",
    "    return pos[1], syl_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_def(word):\n",
    "    build_text = ''\n",
    "    syns = wordnet.synsets(word)\n",
    "    for x in syns:\n",
    "        build_text = build_text + \" \" + x.definition()\n",
    "    tokens = nltk.word_tokenize(build_text)\n",
    "\n",
    "    return build_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = build_def('word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a list of a given part of speech (POS) from text\n",
    "def find_pos_from_text(text, POS):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tagged = nltk.pos_tag(tokens)\n",
    "    pos_list = [x for x in tagged if x[1] == POS]\n",
    "    return [x[0] for x in pos_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a list of a given part of speech (POS) from text\n",
    "def find_poc_from_list(voc_list, POS):\n",
    "    tagged = nltk.pos_tag(voc_list)\n",
    "    pos_list = [x for x in tagged if x[1] == POS]\n",
    "    return [x[0] for x in pos_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "POS, syl_count = syl_count_pos('cat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syl_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_associated_words_same_POS(word, limit=100):\n",
    "    \n",
    "    POS, syl_count = syl_count_pos(word)\n",
    "    \n",
    "    definition = build_def(word)\n",
    "    \n",
    "    associated_POS = find_pos_from_text(definition, POS)\n",
    "    \n",
    "    \n",
    "    final_POS_list = []\n",
    "    \n",
    "    for x in associated_POS:\n",
    "        final_POS_list = final_POS_list +\n",
    "        try:\n",
    "            if syl_count_pos(x) == syl_count:\n",
    "                final_nouns.append(x)\n",
    "        except KeyError:\n",
    "            continue\n",
    "            \n",
    "    if not final_nouns:\n",
    "        return \"spam\"\n",
    "    else:\n",
    "        return noun, random.choice(final_nouns)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def three_similar_ing_verbs(verb_list):\n",
    "    random_ing_verb = random.choice(verb_list)\n",
    "    synonyms = []\n",
    "    for syn in wordnet.synsets(random_ing_verb): \n",
    "        for l in syn.lemmas(): \n",
    "            synonyms.append(l.name())         \n",
    "    synonyms = list(set(synonyms))\n",
    "    tagged = nltk.pos_tag(synonyms)\n",
    "    final_verbs = [y[0] for y in tagged if y[1] == 'VBG']\n",
    "    \n",
    "    # creates artificial verbs\n",
    "    add_ing = [x[0] + 'ing' for x in tagged if x[1] != 'VBG']\n",
    "    \n",
    "    # Eliminates compound words and checks to make sure artificial verbs are legitimate\n",
    "    for z in add_ing:\n",
    "        if '_' in z:\n",
    "            continue\n",
    "        else:\n",
    "            if wordnet.synsets(z):\n",
    "                final_verbs.append(z)\n",
    "    final_verbs = list(set(final_verbs))\n",
    "    return random.sample(final_verbs, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_adjs(definition):\n",
    "    tokens = nltk.word_tokenize(definition)\n",
    "    tagged = nltk.pos_tag(tokens)\n",
    "    adj_list = [x[0] for x in tagged if x[1] == 'JJ']\n",
    "    return list(set(adj_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_similar_adj(adj_list):\n",
    "#     random_adj = random.choice(adj_list)\n",
    "    synonyms = []\n",
    "    for adj in adj_list:\n",
    "        for syn in wordnet.synsets(adj): \n",
    "            for l in syn.lemmas(): \n",
    "                synonyms.append(l.name())         \n",
    "    synonyms = list(set(synonyms))\n",
    "#     return synonyms\n",
    "    tagged = nltk.pos_tag(synonyms)\n",
    "    final_adjs = [y[0] for y in tagged if y[1] == 'JJ']\n",
    "    final_adjs = list(set(final_adjs))\n",
    "    for x in final_adjs:\n",
    "        try:\n",
    "            if syl_count_pos(x)[1] != 2:\n",
    "                \n",
    "            \n",
    "    final_adjs = [x for x in final_adjs if syl_count_pos(x)[1] == 2]\n",
    "    return random.sample(final_adjs, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_list = find_adjs(definition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['able', 'asleep', 'metabolic', 'inactive']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'metabolous'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-89-5460d1fcc774>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtwo_similar_adj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0madj_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-88-a050b607f246>\u001b[0m in \u001b[0;36mtwo_similar_adj\u001b[1;34m(adj_list)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mfinal_adjs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfinal_adjs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mfinal_adjs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfinal_adjs\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0msyl_count_pos\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfinal_adjs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-88-a050b607f246>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mfinal_adjs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfinal_adjs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mfinal_adjs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfinal_adjs\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0msyl_count_pos\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfinal_adjs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-7379b853e1fc>\u001b[0m in \u001b[0;36msyl_count_pos\u001b[1;34m(word)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msyl_count_pos\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0md\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcmudict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0msyl_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misdigit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mtoken\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mpos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'metabolous'"
     ]
    }
   ],
   "source": [
    "two_similar_adj(adj_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = \"Insect Hidden, hungry Preening, searching, stalking Waits as if praying Mantis\"\n",
    "tokens = nltk.word_tokenize(sample)\n",
    "tagged = nltk.pos_tag(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Insect', 'NNP'),\n",
       " ('Hidden', 'NNP'),\n",
       " (',', ','),\n",
       " ('hungry', 'JJ'),\n",
       " ('Preening', 'NNP'),\n",
       " (',', ','),\n",
       " ('searching', 'VBG'),\n",
       " (',', ','),\n",
       " ('stalking', 'VBG'),\n",
       " ('Waits', 'NNS'),\n",
       " ('as', 'IN'),\n",
       " ('if', 'IN'),\n",
       " ('praying', 'VBG'),\n",
       " ('Mantis', 'NN')]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "definition = build_def('sleeping')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['able', 'asleep', 'metabolic', 'inactive']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_adjs(definition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_ing_verbs(definition):\n",
    "    tokens = nltk.word_tokenize(definition)\n",
    "    tagged = nltk.pos_tag(tokens)\n",
    "    verb_list = [x for x in tagged if x[1] == 'VBG']\n",
    "    return [x[0] for x in verb_list]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dog_verbs = find_ing_verbs(build_def('dog'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'_' in 'wipe_outing'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('consume.v.05'),\n",
       " Synset('eliminate.v.03'),\n",
       " Synset('wipe_out.v.03'),\n",
       " Synset('erase.v.01'),\n",
       " Synset('kill.v.12'),\n",
       " Synset('cancel_out.v.01')]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet.synsets('wipe_outing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['falsifying', 'garbleing', 'distorting']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "three_similar_ing_verbs(['warp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['run_throughing',\n",
       " 'wipe_outing',\n",
       " 'use_uping',\n",
       " 'rusting',\n",
       " 'feeding',\n",
       " 'consumeing',\n",
       " 'depleteing',\n",
       " 'exhausting',\n",
       " 'eat_oning',\n",
       " 'corrodeing',\n",
       " 'eat_uping',\n",
       " 'eating']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "three_similar_ing_verbs(['eating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'living'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def three_similar_ing_verbs(verb_list):\n",
    "    final_three = []\n",
    "    for x in range(0, len(verb_list)):\n",
    "        random_ing_verb = random.choice(find_verbs(verb_list))\n",
    "        synonyms = []\n",
    "        for syn in wordnet.synsets(random_ing_verb): \n",
    "            for l in syn.lemmas(): \n",
    "                synonyms.append(l.name())\n",
    "        synonyms = list(set(synonyms))\n",
    "        tagged = nltk.pos_tag(synonyms)\n",
    "        ing_list = [y for y in tagged if y[1] == 'VBG']\n",
    "        final_three.append()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('surviving', 'VBG')]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verb_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged = nltk.pos_tag(synonyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'aliveness',\n",
       " 'animation',\n",
       " 'be',\n",
       " 'bread_and_butter',\n",
       " 'dwell',\n",
       " 'endure',\n",
       " 'exist',\n",
       " 'experience',\n",
       " 'go',\n",
       " 'hold_out',\n",
       " 'hold_up',\n",
       " 'inhabit',\n",
       " 'keep',\n",
       " 'know',\n",
       " 'last',\n",
       " 'life',\n",
       " 'live',\n",
       " 'live_on',\n",
       " 'livelihood',\n",
       " 'living',\n",
       " 'populate',\n",
       " 'subsist',\n",
       " 'support',\n",
       " 'survive',\n",
       " 'surviving',\n",
       " 'sustenance'}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(synonyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['life',\n",
       " 'living',\n",
       " 'living',\n",
       " 'animation',\n",
       " 'life',\n",
       " 'living',\n",
       " 'aliveness',\n",
       " 'support',\n",
       " 'keep',\n",
       " 'livelihood',\n",
       " 'living',\n",
       " 'bread_and_butter',\n",
       " 'sustenance',\n",
       " 'populate',\n",
       " 'dwell',\n",
       " 'live',\n",
       " 'inhabit',\n",
       " 'live',\n",
       " 'survive',\n",
       " 'last',\n",
       " 'live',\n",
       " 'live_on',\n",
       " 'go',\n",
       " 'endure',\n",
       " 'hold_up',\n",
       " 'hold_out',\n",
       " 'exist',\n",
       " 'survive',\n",
       " 'live',\n",
       " 'subsist',\n",
       " 'be',\n",
       " 'live',\n",
       " 'know',\n",
       " 'experience',\n",
       " 'live',\n",
       " 'live',\n",
       " 'living',\n",
       " 'living',\n",
       " 'living',\n",
       " 'surviving',\n",
       " 'living',\n",
       " 'living',\n",
       " 'living']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet \n",
    "synonyms = [] \n",
    "antonyms = [] \n",
    "  \n",
    "for syn in wordnet.synsets(\"gopher\"): \n",
    "    for l in syn.lemmas(): \n",
    "        synonyms.append(l.name()) \n",
    "        if l.antonyms(): \n",
    "            antonyms.append(l.antonyms()[0].name()) \n",
    "  \n",
    "print(set(synonyms)) \n",
    "print(set(antonyms)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = nltk.word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Animal',\n",
       " 'brown',\n",
       " ',',\n",
       " 'fuzzy',\n",
       " ',',\n",
       " 'digging',\n",
       " ',',\n",
       " 'squeeking',\n",
       " ',',\n",
       " 'eating',\n",
       " ',',\n",
       " 'stands',\n",
       " 'as',\n",
       " 'if',\n",
       " 'watching',\n",
       " 'gopher']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged = nltk.pos_tag(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Animal', 'NNP'),\n",
       " ('brown', 'NN'),\n",
       " (',', ','),\n",
       " ('fuzzy', 'NN'),\n",
       " (',', ','),\n",
       " ('digging', 'NN'),\n",
       " (',', ','),\n",
       " ('squeeking', 'VBG'),\n",
       " (',', ','),\n",
       " ('eating', 'VBG'),\n",
       " (',', ','),\n",
       " ('stands', 'VBZ'),\n",
       " ('as', 'IN'),\n",
       " ('if', 'IN'),\n",
       " ('watching', 'VBG'),\n",
       " ('gopher', 'NN')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def syl_count_pos(word):\n",
    "    d = cmudict.dict()\n",
    "    syl_count = [len(list(y for y in x if y[-1].isdigit())) for x in d[word.lower()]][0]\n",
    "    token = nltk.word_tokenize(word)\n",
    "    pos = nltk.pos_tag(token)[0]\n",
    "    return pos[1], syl_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('NN', 3)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syl_count_pos('Christopher')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gopher', 'gopher_turtle', 'Gopherus_polypemus', 'pocket_gopher', 'Gopher', 'Minnesotan', 'pouched_rat', 'ground_squirrel', 'gopher_tortoise', 'spermophile', 'goffer'}\n",
      "set()\n"
     ]
    }
   ],
   "source": [
    "import nltk \n",
    "from nltk.corpus import wordnet \n",
    "synonyms = [] \n",
    "antonyms = [] \n",
    "  \n",
    "for syn in wordnet.synsets(\"gopher\"): \n",
    "    for l in syn.lemmas(): \n",
    "        synonyms.append(l.name()) \n",
    "        if l.antonyms(): \n",
    "            antonyms.append(l.antonyms()[0].name()) \n",
    "  \n",
    "print(set(synonyms)) \n",
    "print(set(antonyms)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No matches\n"
     ]
    }
   ],
   "source": [
    "text = nltk.Text(word.lower() for word in nltk.corpus.brown.words())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time name man be work face day school return show life car and for do\n",
      "head house him me position\n"
     ]
    }
   ],
   "source": [
    "text.similar('smile')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster:\n",
      "Cluster 0:\n",
      " google\n",
      " cat\n",
      " best\n",
      " climbing\n",
      " ninja\n",
      " incredible\n",
      " app\n",
      " translate\n",
      " impressed\n",
      " map\n",
      "Cluster 1:\n",
      " eating\n",
      " kitty\n",
      " little\n",
      " came\n",
      " restaurant\n",
      " play\n",
      " ve\n",
      " feedback\n",
      " face\n",
      " extension\n",
      "\n",
      "\n",
      "Prediction\n",
      "[0]\n",
      "[0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "\n",
    "documents = [\"This little kitty came to play when I was eating at a restaurant.\",\n",
    "             \"Merley has the best squooshy kitten belly.\",\n",
    "             \"Google Translate app is incredible.\",\n",
    "             \"If you open 100 tab in google you get a smiley face.\",\n",
    "             \"Best cat photo I've ever taken.\",\n",
    "             \"Climbing ninja cat.\",\n",
    "             \"Impressed with google map feedback.\",\n",
    "             \"Key promoter extension for Google Chrome.\"]\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(documents)\n",
    "\n",
    "true_k = 2\n",
    "model = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n",
    "model.fit(X)\n",
    "\n",
    "print(\"Top terms per cluster:\")\n",
    "order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names()\n",
    "for i in range(true_k):\n",
    "    print(\"Cluster %d:\" % i),\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print(' %s' % terms[ind]),\n",
    "    print\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Prediction\")\n",
    "\n",
    "Y = vectorizer.transform([\"chrome browser to open.\"])\n",
    "prediction = model.predict(Y)\n",
    "print(prediction)\n",
    "\n",
    "Y = vectorizer.transform([\"My cat is hungry.\"])\n",
    "prediction = model.predict(Y)\n",
    "print(prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

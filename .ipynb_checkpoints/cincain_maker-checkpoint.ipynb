{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import cmudict\n",
    "import random\n",
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomtext():\n",
    "    return f\"text{random.randint(1,9)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'text2'"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randomtext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = 'wicked'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.similar(wicked)'"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'randint'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-182-99569dfddbc5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprogram\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mf\"{randomtext()}{similar_text}\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mexec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprogram\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-181-7cd723e43806>\u001b[0m in \u001b[0;36mrandomtext\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mrandomtext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[1;34mf\"text{random.randint(1,9)}\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'randint'"
     ]
    }
   ],
   "source": [
    "program = f\"{randomtext()}{similar_text}\"\n",
    "exec(program)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similar_random(word):\n",
    "    similar_text = f\".similar('{word}')\"\n",
    "    int = random.randint(1,9)\n",
    "    program = f\"text{int}{similar_text}\"\n",
    "    exec(program)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "whale sea water same last lines whole heart world hand english full\n",
      "deep lord land other waves king tail side\n"
     ]
    }
   ],
   "source": [
    "dip = similar_random('blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similar_random2(word):\n",
    "    similar_text = f\".similar('{word}')\"\n",
    "    int = random.randint(1,9)\n",
    "    program = f\"text{int}{similar_text}\"\n",
    "    exec(program)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "white original whitte round cigars\n"
     ]
    }
   ],
   "source": [
    "dip = similar_random2('blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120\n"
     ]
    }
   ],
   "source": [
    "def exec_code(): \n",
    "    LOC = \"\"\" \n",
    "def factorial(num): \n",
    "    fact=1 \n",
    "    for i in range(1,num+1): \n",
    "        fact = fact*i \n",
    "    return fact \n",
    "print(factorial(5)) \n",
    "\"\"\"\n",
    "    exec(LOC) \n",
    "      \n",
    "# Driver Code \n",
    "exec_code() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum = 15\n"
     ]
    }
   ],
   "source": [
    "program = 'a = 5\\nb=10\\nprint(\"Sum =\", a+b)'\n",
    "exec(program)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "program = 'a = 5\\nb=10\\nprint(\"Sum =\", a+b)'\n",
    "exec(program)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "very so exceedingly heartily a as good great extremely remarkably\n",
      "sweet vast amazingly\n"
     ]
    }
   ],
   "source": [
    "text2.similar(\"monstrous\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the\\n'"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# redirect sys.stdout to a buffer\n",
    "import sys, io\n",
    "stdout = sys.stdout\n",
    "sys.stdout = io.StringIO()\n",
    "\n",
    "# call module that calls print()\n",
    "text2.similar(\"blue\")\n",
    "\n",
    "# get output and restore sys.stdout\n",
    "output = sys.stdout.getvalue()\n",
    "sys.stdout = stdout\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the\\n'"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no matches\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text3.concordance(\"borf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Text: Monty Python and the Holy Grail>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def syl_count_pos(word):\n",
    "    d = cmudict.dict()\n",
    "    syl_count = [len(list(y for y in x if y[-1].isdigit())) for x in d[word.lower()]][0]\n",
    "    token = nltk.word_tokenize(word)\n",
    "    pos = nltk.pos_tag(token)[0]\n",
    "    return pos[1], syl_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_def(word):\n",
    "    build_text = ''\n",
    "    syns = wordnet.synsets(word)\n",
    "    for x in syns:\n",
    "        build_text = build_text + \" \" + x.definition()\n",
    "    tokens = nltk.word_tokenize(build_text)\n",
    "\n",
    "    return build_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nouns(definition):\n",
    "    tokens = nltk.word_tokenize(definition)\n",
    "    tagged = nltk.pos_tag(tokens)\n",
    "    noun_list = [x for x in tagged if x[1] == 'NN']\n",
    "    return [x[0] for x in noun_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_ing_verbs(definition):\n",
    "    tokens = nltk.word_tokenize(definition)\n",
    "    tagged = nltk.pos_tag(tokens)\n",
    "    verb_list = [x for x in tagged if x[1] == 'VBG']\n",
    "    return [x[0] for x in verb_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_associated_noun(noun):\n",
    "    syl_count = syl_count_pos(noun)\n",
    "    definition = build_def(noun)\n",
    "    associated_nouns = find_nouns(definition)\n",
    "    \n",
    "    final_nouns = []\n",
    "    for x in associated_nouns:\n",
    "        try:\n",
    "            if syl_count_pos(x) == syl_count:\n",
    "                final_nouns.append(x)\n",
    "        except KeyError:\n",
    "            continue\n",
    "            \n",
    "    if not final_nouns:\n",
    "        return \"spam\"\n",
    "    else:\n",
    "        return noun, random.choice(final_nouns)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def three_similar_ing_verbs(verb_list):\n",
    "    random_ing_verb = random.choice(verb_list)\n",
    "    synonyms = []\n",
    "    for syn in wordnet.synsets(random_ing_verb): \n",
    "        for l in syn.lemmas(): \n",
    "            synonyms.append(l.name())         \n",
    "    synonyms = list(set(synonyms))\n",
    "    tagged = nltk.pos_tag(synonyms)\n",
    "    final_verbs = [y[0] for y in tagged if y[1] == 'VBG']\n",
    "    \n",
    "    # creates artificial verbs\n",
    "    add_ing = [x[0] + 'ing' for x in tagged if x[1] != 'VBG']\n",
    "    \n",
    "    # Eliminates compound words and checks to make sure artificial verbs are legitimate\n",
    "    for z in add_ing:\n",
    "        if '_' in z:\n",
    "            continue\n",
    "        else:\n",
    "            if wordnet.synsets(z):\n",
    "                final_verbs.append(z)\n",
    "    final_verbs = list(set(final_verbs))\n",
    "    return random.sample(final_verbs, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_adjs(definition):\n",
    "    tokens = nltk.word_tokenize(definition)\n",
    "    tagged = nltk.pos_tag(tokens)\n",
    "    adj_list = [x[0] for x in tagged if x[1] == 'JJ']\n",
    "    return list(set(adj_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_similar_adj(adj_list):\n",
    "#     random_adj = random.choice(adj_list)\n",
    "    synonyms = []\n",
    "    for adj in adj_list:\n",
    "        for syn in wordnet.synsets(adj): \n",
    "            for l in syn.lemmas(): \n",
    "                synonyms.append(l.name())         \n",
    "    synonyms = list(set(synonyms))\n",
    "#     return synonyms\n",
    "    tagged = nltk.pos_tag(synonyms)\n",
    "    final_adjs = [y[0] for y in tagged if y[1] == 'JJ']\n",
    "    final_adjs = list(set(final_adjs))\n",
    "    for x in final_adjs:\n",
    "        try:\n",
    "            if syl_count_pos(x)[1] != 2:\n",
    "                \n",
    "            \n",
    "    final_adjs = [x for x in final_adjs if syl_count_pos(x)[1] == 2]\n",
    "    return random.sample(final_adjs, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_list = find_adjs(definition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['able', 'asleep', 'metabolic', 'inactive']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'metabolous'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-89-5460d1fcc774>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtwo_similar_adj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0madj_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-88-a050b607f246>\u001b[0m in \u001b[0;36mtwo_similar_adj\u001b[1;34m(adj_list)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mfinal_adjs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfinal_adjs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mfinal_adjs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfinal_adjs\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0msyl_count_pos\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfinal_adjs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-88-a050b607f246>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mfinal_adjs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfinal_adjs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mfinal_adjs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfinal_adjs\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0msyl_count_pos\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfinal_adjs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-7379b853e1fc>\u001b[0m in \u001b[0;36msyl_count_pos\u001b[1;34m(word)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msyl_count_pos\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0md\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcmudict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0msyl_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misdigit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mtoken\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mpos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'metabolous'"
     ]
    }
   ],
   "source": [
    "two_similar_adj(adj_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = \"Insect Hidden, hungry Preening, searching, stalking Waits as if praying Mantis\"\n",
    "tokens = nltk.word_tokenize(sample)\n",
    "tagged = nltk.pos_tag(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Insect', 'NNP'),\n",
       " ('Hidden', 'NNP'),\n",
       " (',', ','),\n",
       " ('hungry', 'JJ'),\n",
       " ('Preening', 'NNP'),\n",
       " (',', ','),\n",
       " ('searching', 'VBG'),\n",
       " (',', ','),\n",
       " ('stalking', 'VBG'),\n",
       " ('Waits', 'NNS'),\n",
       " ('as', 'IN'),\n",
       " ('if', 'IN'),\n",
       " ('praying', 'VBG'),\n",
       " ('Mantis', 'NN')]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "definition = build_def('sleeping')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['able', 'asleep', 'metabolic', 'inactive']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_adjs(definition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_ing_verbs(definition):\n",
    "    tokens = nltk.word_tokenize(definition)\n",
    "    tagged = nltk.pos_tag(tokens)\n",
    "    verb_list = [x for x in tagged if x[1] == 'VBG']\n",
    "    return [x[0] for x in verb_list]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dog_verbs = find_ing_verbs(build_def('dog'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'_' in 'wipe_outing'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('consume.v.05'),\n",
       " Synset('eliminate.v.03'),\n",
       " Synset('wipe_out.v.03'),\n",
       " Synset('erase.v.01'),\n",
       " Synset('kill.v.12'),\n",
       " Synset('cancel_out.v.01')]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet.synsets('wipe_outing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['falsifying', 'garbleing', 'distorting']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "three_similar_ing_verbs(['warp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['run_throughing',\n",
       " 'wipe_outing',\n",
       " 'use_uping',\n",
       " 'rusting',\n",
       " 'feeding',\n",
       " 'consumeing',\n",
       " 'depleteing',\n",
       " 'exhausting',\n",
       " 'eat_oning',\n",
       " 'corrodeing',\n",
       " 'eat_uping',\n",
       " 'eating']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "three_similar_ing_verbs(['eating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'living'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def three_similar_ing_verbs(verb_list):\n",
    "    final_three = []\n",
    "    for x in range(0, len(verb_list)):\n",
    "        random_ing_verb = random.choice(find_verbs(verb_list))\n",
    "        synonyms = []\n",
    "        for syn in wordnet.synsets(random_ing_verb): \n",
    "            for l in syn.lemmas(): \n",
    "                synonyms.append(l.name())\n",
    "        synonyms = list(set(synonyms))\n",
    "        tagged = nltk.pos_tag(synonyms)\n",
    "        ing_list = [y for y in tagged if y[1] == 'VBG']\n",
    "        final_three.append()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('surviving', 'VBG')]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verb_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged = nltk.pos_tag(synonyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'aliveness',\n",
       " 'animation',\n",
       " 'be',\n",
       " 'bread_and_butter',\n",
       " 'dwell',\n",
       " 'endure',\n",
       " 'exist',\n",
       " 'experience',\n",
       " 'go',\n",
       " 'hold_out',\n",
       " 'hold_up',\n",
       " 'inhabit',\n",
       " 'keep',\n",
       " 'know',\n",
       " 'last',\n",
       " 'life',\n",
       " 'live',\n",
       " 'live_on',\n",
       " 'livelihood',\n",
       " 'living',\n",
       " 'populate',\n",
       " 'subsist',\n",
       " 'support',\n",
       " 'survive',\n",
       " 'surviving',\n",
       " 'sustenance'}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(synonyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['life',\n",
       " 'living',\n",
       " 'living',\n",
       " 'animation',\n",
       " 'life',\n",
       " 'living',\n",
       " 'aliveness',\n",
       " 'support',\n",
       " 'keep',\n",
       " 'livelihood',\n",
       " 'living',\n",
       " 'bread_and_butter',\n",
       " 'sustenance',\n",
       " 'populate',\n",
       " 'dwell',\n",
       " 'live',\n",
       " 'inhabit',\n",
       " 'live',\n",
       " 'survive',\n",
       " 'last',\n",
       " 'live',\n",
       " 'live_on',\n",
       " 'go',\n",
       " 'endure',\n",
       " 'hold_up',\n",
       " 'hold_out',\n",
       " 'exist',\n",
       " 'survive',\n",
       " 'live',\n",
       " 'subsist',\n",
       " 'be',\n",
       " 'live',\n",
       " 'know',\n",
       " 'experience',\n",
       " 'live',\n",
       " 'live',\n",
       " 'living',\n",
       " 'living',\n",
       " 'living',\n",
       " 'surviving',\n",
       " 'living',\n",
       " 'living',\n",
       " 'living']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('life.n.02'),\n",
       " Synset('living.n.02'),\n",
       " Synset('animation.n.01'),\n",
       " Synset('support.n.06'),\n",
       " Synset('populate.v.01'),\n",
       " Synset('live.v.02'),\n",
       " Synset('survive.v.01'),\n",
       " Synset('exist.v.02'),\n",
       " Synset('be.v.11'),\n",
       " Synset('know.v.05'),\n",
       " Synset('live.v.07'),\n",
       " Synset('living.a.01'),\n",
       " Synset('living.s.02'),\n",
       " Synset('living.s.03'),\n",
       " Synset('surviving.s.01'),\n",
       " Synset('living.s.05'),\n",
       " Synset('living.s.06')]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet.synsets(random_ing_verb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet \n",
    "synonyms = [] \n",
    "antonyms = [] \n",
    "  \n",
    "for syn in wordnet.synsets(\"gopher\"): \n",
    "    for l in syn.lemmas(): \n",
    "        synonyms.append(l.name()) \n",
    "        if l.antonyms(): \n",
    "            antonyms.append(l.antonyms()[0].name()) \n",
    "  \n",
    "print(set(synonyms)) \n",
    "print(set(antonyms)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mammal',\n",
       " 'fur',\n",
       " 'ability',\n",
       " 'term',\n",
       " 'youth',\n",
       " 'man',\n",
       " 'woman',\n",
       " 'shrub',\n",
       " 'edulis',\n",
       " 'tobacco',\n",
       " 'tea',\n",
       " 'effect',\n",
       " 'stimulant',\n",
       " 'whip',\n",
       " 'vehicle',\n",
       " 'metal',\n",
       " 'earth',\n",
       " 'construction',\n",
       " 'farm',\n",
       " 'work',\n",
       " 'wild',\n",
       " 'method',\n",
       " 'body',\n",
       " 'computer',\n",
       " 'series',\n",
       " 'axis',\n",
       " 'beat',\n",
       " 'eject',\n",
       " 'stomach',\n",
       " 'mouth']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_nouns(cat_def)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_def = build_def('cat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = nltk.word_tokenize(cat_def)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged = nltk.pos_tag(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('mammal', 'NN'),\n",
       " ('fur', 'NN'),\n",
       " ('ability', 'NN'),\n",
       " ('term', 'NN'),\n",
       " ('youth', 'NN'),\n",
       " ('man', 'NN'),\n",
       " ('woman', 'NN'),\n",
       " ('shrub', 'NN'),\n",
       " ('edulis', 'NN'),\n",
       " ('tobacco', 'NN'),\n",
       " ('tea', 'NN'),\n",
       " ('effect', 'NN'),\n",
       " ('stimulant', 'NN'),\n",
       " ('whip', 'NN'),\n",
       " ('vehicle', 'NN'),\n",
       " ('metal', 'NN'),\n",
       " ('earth', 'NN'),\n",
       " ('construction', 'NN'),\n",
       " ('farm', 'NN'),\n",
       " ('work', 'NN'),\n",
       " ('wild', 'NN'),\n",
       " ('method', 'NN'),\n",
       " ('body', 'NN'),\n",
       " ('computer', 'NN'),\n",
       " ('series', 'NN'),\n",
       " ('axis', 'NN'),\n",
       " ('beat', 'NN'),\n",
       " ('eject', 'NN'),\n",
       " ('stomach', 'NN'),\n",
       " ('mouth', 'NN')]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x for x in tagged if x[1] == 'NN']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = nltk.word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('goffer.n.01'),\n",
       " Synset('minnesotan.n.01'),\n",
       " Synset('ground_squirrel.n.02'),\n",
       " Synset('gopher.n.04'),\n",
       " Synset('gopher_tortoise.n.01')]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = nltk.word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Animal',\n",
       " 'brown',\n",
       " ',',\n",
       " 'fuzzy',\n",
       " ',',\n",
       " 'digging',\n",
       " ',',\n",
       " 'squeeking',\n",
       " ',',\n",
       " 'eating',\n",
       " ',',\n",
       " 'stands',\n",
       " 'as',\n",
       " 'if',\n",
       " 'watching',\n",
       " 'gopher']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged = nltk.pos_tag(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Animal', 'NNP'),\n",
       " ('brown', 'NN'),\n",
       " (',', ','),\n",
       " ('fuzzy', 'NN'),\n",
       " (',', ','),\n",
       " ('digging', 'NN'),\n",
       " (',', ','),\n",
       " ('squeeking', 'VBG'),\n",
       " (',', ','),\n",
       " ('eating', 'VBG'),\n",
       " (',', ','),\n",
       " ('stands', 'VBZ'),\n",
       " ('as', 'IN'),\n",
       " ('if', 'IN'),\n",
       " ('watching', 'VBG'),\n",
       " ('gopher', 'NN')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> \n",
    ">>> tokens\n",
    "['At', 'eight', \"o'clock\", 'on', 'Thursday', 'morning',\n",
    "'Arthur', 'did', \"n't\", 'feel', 'very', 'good', '.']\n",
    ">>> tagged = nltk.pos_tag(tokens)\n",
    ">>> tagged[0:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def syl_count_pos(word):\n",
    "    d = cmudict.dict()\n",
    "    syl_count = [len(list(y for y in x if y[-1].isdigit())) for x in d[word.lower()]][0]\n",
    "    token = nltk.word_tokenize(word)\n",
    "    pos = nltk.pos_tag(token)[0]\n",
    "    return pos[1], syl_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('NN', 3)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syl_count_pos('Christopher')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gopher', 'gopher_turtle', 'Gopherus_polypemus', 'pocket_gopher', 'Gopher', 'Minnesotan', 'pouched_rat', 'ground_squirrel', 'gopher_tortoise', 'spermophile', 'goffer'}\n",
      "set()\n"
     ]
    }
   ],
   "source": [
    "import nltk \n",
    "from nltk.corpus import wordnet \n",
    "synonyms = [] \n",
    "antonyms = [] \n",
    "  \n",
    "for syn in wordnet.synsets(\"gopher\"): \n",
    "    for l in syn.lemmas(): \n",
    "        synonyms.append(l.name()) \n",
    "        if l.antonyms(): \n",
    "            antonyms.append(l.antonyms()[0].name()) \n",
    "  \n",
    "print(set(synonyms)) \n",
    "print(set(antonyms)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No matches\n"
     ]
    }
   ],
   "source": [
    "text = nltk.Text(word.lower() for word in nltk.corpus.brown.words())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time name man be work face day school return show life car and for do\n",
      "head house him me position\n"
     ]
    }
   ],
   "source": [
    "text.similar('smile')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster:\n",
      "Cluster 0:\n",
      " google\n",
      " cat\n",
      " best\n",
      " climbing\n",
      " ninja\n",
      " incredible\n",
      " app\n",
      " translate\n",
      " impressed\n",
      " map\n",
      "Cluster 1:\n",
      " eating\n",
      " kitty\n",
      " little\n",
      " came\n",
      " restaurant\n",
      " play\n",
      " ve\n",
      " feedback\n",
      " face\n",
      " extension\n",
      "\n",
      "\n",
      "Prediction\n",
      "[0]\n",
      "[0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "\n",
    "documents = [\"This little kitty came to play when I was eating at a restaurant.\",\n",
    "             \"Merley has the best squooshy kitten belly.\",\n",
    "             \"Google Translate app is incredible.\",\n",
    "             \"If you open 100 tab in google you get a smiley face.\",\n",
    "             \"Best cat photo I've ever taken.\",\n",
    "             \"Climbing ninja cat.\",\n",
    "             \"Impressed with google map feedback.\",\n",
    "             \"Key promoter extension for Google Chrome.\"]\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(documents)\n",
    "\n",
    "true_k = 2\n",
    "model = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n",
    "model.fit(X)\n",
    "\n",
    "print(\"Top terms per cluster:\")\n",
    "order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names()\n",
    "for i in range(true_k):\n",
    "    print(\"Cluster %d:\" % i),\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print(' %s' % terms[ind]),\n",
    "    print\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Prediction\")\n",
    "\n",
    "Y = vectorizer.transform([\"chrome browser to open.\"])\n",
    "prediction = model.predict(Y)\n",
    "print(prediction)\n",
    "\n",
    "Y = vectorizer.transform([\"My cat is hungry.\"])\n",
    "prediction = model.predict(Y)\n",
    "print(prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
